{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee0956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "N_HAND_LANDMARKS = 21                                       # Using all hand landmarks (https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker)\n",
    "UPPER_BODY_CONNECTIONS = [                                  # Using upper body landmarks only without hand landmarks (https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker)\n",
    "    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
    "    10, 11, 12, 13, 14, 23, 24\n",
    "] \n",
    "N_POSE_LANDMARKS = len(UPPER_BODY_CONNECTIONS)\n",
    "N_LANDMARKS = N_POSE_LANDMARKS + N_HAND_LANDMARKS*2         # Total number of landmarks for upper body and two hands (left + right)\n",
    "K = 20\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"Convert color space and run Mediapipe model.\"\"\"\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def extract_keypoints(results, visibility_thres=0.5):\n",
    "    \"\"\"Extract all keypoints from one video frame.\"\"\"\n",
    "    \n",
    "    # Upper pose\n",
    "    pose_landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        for idx, i in enumerate(UPPER_BODY_CONNECTIONS):\n",
    "            if i < len(landmarks):\n",
    "                res = landmarks[i]\n",
    "                if res.visibility < visibility_thres:         \n",
    "                    pose_landmarks.append([0.0, 0.0, 0.0])\n",
    "                else:\n",
    "                    pose_landmarks.append([res.x, res.y, res.z])\n",
    "    else:\n",
    "        pose_landmarks = [[0.0, 0.0, 0.0]] * N_POSE_LANDMARKS\n",
    "    \n",
    "    # Left hand\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand_landmarks = [[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]\n",
    "    else:\n",
    "        left_hand_landmarks = [[0.0, 0.0, 0.0]] * N_HAND_LANDMARKS\n",
    "\n",
    "    # Right hand\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand_landmarks = [[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]\n",
    "    else:\n",
    "        right_hand_landmarks = [[0.0, 0.0, 0.0]] * N_HAND_LANDMARKS\n",
    "    \n",
    "    return pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "\n",
    "\n",
    "def plot_keypoints(list_landmarks):\n",
    "    \"\"\"Plot image with keypoints\"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761324934.465622  435233 gl_context.cc:369] GL version: 2.1 (2.1 INTEL-20.7.3), renderer: Intel(R) Iris(TM) Plus Graphics 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1761324935.522528  435788 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761324935.709793  435788 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761324935.722274  435788 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761324935.724263  435787 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761324935.819915  435790 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761324935.871013  435790 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761324936.019373  435789 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761324936.183576  435788 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_holistic = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def get_list_frame(source_path):\n",
    "    \"\"\"Extract keypoints sequences from one video.\"\"\"\n",
    "    \n",
    "    frames_keypoints = []\n",
    "    cap = cv2.VideoCapture(source_path)\n",
    "    while cap.isOpened():\n",
    "        # read video frame\n",
    "        success, image = cap.read()\n",
    "\n",
    "        # skip empty frames\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        # MediaPipe Holistic processing\n",
    "        _, results = md.mediapipe_detection(image, mp_holistic)\n",
    "        pose_landmarks, left_hand_landmarks, right_hand_landmarks = md.extract_keypoints(results)\n",
    "        \n",
    "        frames_keypoints.append([left_hand_landmarks, right_hand_landmarks, pose_landmarks])\n",
    "    cap.release()\n",
    "    return frames_keypoints\n",
    "    \n",
    "def concate_array(left_hand_landmarks, right_hand_landmarks, pose_landmarks):\n",
    "    \"\"\" \"\"\"\n",
    "    a1 = np.array(left_hand_landmarks).reshape(-1)\n",
    "    a2 = np.array(right_hand_landmarks).reshape(-1)\n",
    "    a3 = np.array(pose_landmarks).reshape(-1)\n",
    "    result = np.concatenate((a1, a2, a3), axis=None)\n",
    "    return result\n",
    "\n",
    "def check_zeros(list_landmarks):\n",
    "    data = np.array(list_landmarks)\n",
    "    if np.all(data == 0):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def write_data(output_dir, source_path, file_name):\n",
    "    \"\"\"Write keypoints sequence into numpy file from original video.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        list_fr = get_list_frame(source_path)\n",
    "        X = []\n",
    "        list_idx = []\n",
    "        for i in range(len(list_fr)):\n",
    "            if check_zeros(list_fr[i][0]) and check_zeros(list_fr[i][1]):\n",
    "                continue\n",
    "            X.append(concate_array(list_fr[i][0], list_fr[i][1], list_fr[i][2]))\n",
    "            list_idx.append(i)\n",
    "        if len(X) == 0:\n",
    "            print(\"no valid frame to save in: \" + source_path)\n",
    "            return\n",
    "        \n",
    "        # filtering frame\n",
    "        X_new = np.array(X)\n",
    "        kmeans = KMeans(n_clusters=K)\n",
    "        kmeans.fit(X_new)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        distances = cdist(X_new, cluster_centers, 'euclidean')\n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        index = np.sort(nearest_indices)\n",
    "        \n",
    "        data = []\n",
    "        for i in index:\n",
    "            data.append(list_fr[list_idx[i]])\n",
    "        data_save = np.asarray(data, dtype=\"object\")\n",
    "        np.save(os.path.join(output_dir, file_name), data_save)\n",
    "        print(\"ok write npy from file: \" + source_path) \n",
    "    except Exception as e:\n",
    "        print(f\"error write: {source_path} with {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a1bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path, directory_name):\n",
    "    # Combine the path and directory name\n",
    "    directory_path = os.path.join(path, directory_name)\n",
    "\n",
    "    try:\n",
    "        # Create the directory\n",
    "        os.mkdir(directory_path)\n",
    "        print(\"Directory created successfully!\")\n",
    "    except FileExistsError:\n",
    "        print(\"Directory already exists!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a76d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../notebooks/dataset/videos\"\n",
    "output_dir = \"../notebooks/dataset/keypoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "import json\n",
    "METADATA_PATH = \"../dataset/metadata_v2.jsonl\"\n",
    "\n",
    "with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        if i == 20:\n",
    "            break\n",
    "        data = json.loads(line.strip())\n",
    "        word = data[\"word\"].strip()\n",
    "        dir_path = os.path.join(output_dir, str(word))\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sub_dir in os.listdir(output_dir):\n",
    "#     path = os.path.join(output_dir, sub_dir)\n",
    "#     if os.path.isdir(path):\n",
    "#         new_path = os.path.join(output_dir, sub_dir)\n",
    "#         list_file = list_files(path)\n",
    "#         for i in range(len(list_file)):\n",
    "#             path_to_file = os.path.join(path, list_file[i])\n",
    "#             write_data(new_path, path_to_file, str(i) + \".npy\")\n",
    "                \n",
    "# write_data(output_dir, source_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63067a89",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nauxqouh/mediapipeprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
